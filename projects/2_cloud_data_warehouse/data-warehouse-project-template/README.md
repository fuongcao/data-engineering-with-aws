## Udacity Data Engineering Nanodegree

## Project 2 - Data Warehouse

### Project Description

In the project 2 - Data Warehouse, we'll build an ETL pipeline for a database hosted on Redshift. To complete the project, we need to load data from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from these staging tables.

| <img alt="System Architecture - AWS S3 to RedShift ETL" src="./images/system_architecture.png" width=75% height=75%> |
| :------------------------------------------------------------------------------------------------------------------: |
|                                    _System Architecture - AWS S3 to RedShift ETL_                                    |

---

### Project dataset:

we'll be working with two datasets stored in S3 links:

- Song data: `s3://udacity-dend/song_data`
- Lod data: `s3://udacity-dend/log_data`

To properly read log data `s3://udacity-dend/log_data`, we need the following metadata file:

- Log metadata: `s3://udacity-dend/log_json_path.json`

```
Note: `udacity-dend` bucket is situated in the `us-west-2` region.
```

### Song Dataset

Dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song.

```
[song_data/A/A/A/TRAAAAK128F9318786.json](sample_data/TRAAAAK128F9318786.json)
```

The example for the content of a song file, TRAAAAK128F9318786.json:

```
{"artist_id":"ARJNIUY12298900C91","artist_latitude":null,"artist_location":"","artist_longitude":null,"artist_name":"Adelitas Way","duration":213.9424,"num_songs":1,"song_id":"SOBLFFE12AF72AA5BA","title":"Scream","year":2009}
```

### Log Dataset

Second dataset consists of log files in JSON format generated by this event simulator(opens in a new tab) based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.
For example:

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

| <img alt="log_data sample" src="./images/log_data_sample.png" width=75% height=75%> |
| :---------------------------------------------------------------------------------: |
|                                     _log_data_                                      |

### Log JSON Metadata

The [log_json_path.json](./sample_data/log_json_path.json) file is used when loading JSON data into Redshift. It specifies the structure of the JSON data so that Redshift can properly parse and load it into the staging tables. [log_json_path.json](./sample_data/log_json_path.json) file tells Redshift how to interpret the JSON data and extract the relevant fields. This is essential for further processing and transforming the data into the desired analytics tables.

| <img alt="log-json-path sample" src="./images/log_json_path_content.png"> |
| :-----------------------------------------------------------------------: |
|                     _Content of `log_json_path.json`_                     |

---

### Project Steps

##### 1. Create Table Schema

1. Design Schemas for the fact and dimemsion tables.
2. Write a SQL `CREATE`statement for eachof these tables in [sql_queries.py](sql_queries.py).
3. Complete the logic in [create_tables.py](create_tables.py) to connect to the database and create these tables.
4. Write the `DROP` statements to drop the tables in the beginning of [create_tables.py](create_tables.py)if the table aldready exist.This way, we can run [create_tables.py] wheneverwe wantto resetour databaseand test ETL pipeline.
5. Launch redshift cluster and create a IAM role that has read access to s3.
   - IAM role: `project2RedshiftRole`
6. Add redshift cluster and IAM role info to [dwh.cfg](dwh.cfg).
7. Test by running [create_tables.py](create_tables.py) and checkingthe table schemas in the redshift database. We can use QueryEditor in AWS RedShift console for this.

#### 2. Build ETL Pipeline

1. Implement the logic in [etl.py](etl.py) to load data from S3 to staging tables on RedShift.
2. Implement the logic in [etl.py](etl.py) to load data from staging tables to analytics tables on RedShift.
3. Test by running [etl.py](etl.py) after running [create_tables.py](create_tables.py) and running the analytic queryon the RedShift database to compare the result with the expected results.
4. Delete RedShift cluster when finished.

---

### Process of Project

---

#### 1.1 Design Schemas for the fact and dimemsion tables.

The
Star schemas for Sparikfy for Song play data that tructured by a fact table and 4 dimension tables

Fact table:

> _songplays_: provide the event data associated wit song plays records.
>
> ```
> Columns: songplay_id, user_id, session_id, song_id, artist_id, start_time, level, location, user_agent
> ```

Dimension table:

> _users_: provide the users in the app records.
>
> ```
> Columns: user_id, first_name, last_name, gender, level
> ```

> _songs_: provide the songs in music database records .
>
> ```
> Columns: song_id, title, artist_id, year, duration
> ```

> _artists_: provide the artists in music database records.
>
> ```
> Columns: artist_id, name, location, lattitude, longitude
> ```

> _time_: provide the timestamps records in songplays broken down into specific unit.
>
> ```
> Columns: start_time, hour, day, week, month, year, weekday
> ```

| <img alt="table schemas sample" src="./images/table-schemas.png" width=75% height=75%> |
| :------------------------------------------------------------------------------------: |
|                                    _Table schemas_                                     |

---

#### 1.2 Write a SQL CREATEstatement for eachof these tables in sql_queries.py
